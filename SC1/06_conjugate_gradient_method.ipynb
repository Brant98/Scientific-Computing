{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> ILI285 - Computación Científica I  / INF285 - Computación Científica </h1>\n",
    "    <h2> Conjugate Gradient Method </h2>\n",
    "    <h2> [[S]cientific [C]omputing [T]eam](#acknowledgements)</h2>\n",
    "    <h2> Version: 1.11</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Gradient Descent](#GDragon)\n",
    "* [Conjugate Gradient Method](#CGM)\n",
    "* [Conjugate Gradient Method with Preconditioning](#CGMp)\n",
    "* [Let's Play: Practical Exercises and Profiling](#LP)\n",
    "* [Acknowledgements](#acknowledgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import solve_triangular\n",
    "%matplotlib inline\n",
    "# pip install memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='intro' />\n",
    "## Introduction\n",
    "\n",
    "Welcome to another edition of our IPython Notebooks. Here, we'll teach you how to solve $A\\,x = b$ with $A$ being a _symmetric positive-definite matrix_, but the following methods have a key difference with the previous ones: these do not depend on a matrix factorization. The two methods that we'll see are called the Gradient Descent and the Conjugate Gradient Method. On the latter, we'll also see the benefits of preconditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GDragon' />\n",
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an iterative method. If you remember the iterative methods in the previous Notebook, to find the next approximate solution $\\vec{x}_{k+1}$ you'd add a vector to the current approximate solution, $\\vec{x}_k$, that is: $\\vec{x}_{k+1} = \\vec{x}_k + \\text{vector}$. In this method, $\\text{vector}$ is $\\alpha_{k}\\,\\vec{r}_k$, where $\\vec{r}_k$ is the residue ($\\vec{b} - A\\,\\vec{x}_k$) and $\\alpha_k = \\cfrac{(\\vec{r}_k)^T\\,\\vec{r}_k}{(\\vec{r}_k)^T\\,A\\,\\vec{r}_k}$, starting with some initial guess $\\vec{x}_0$. Let's look at the implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(A, b, x0, n_iter=10):\n",
    "    n = A.shape[0]\n",
    "    #array with solutions\n",
    "    X = np.empty((n_iter, n))\n",
    "    X[0] = x0\n",
    "\n",
    "    for k in range(1, n_iter):\n",
    "        r = b - np.dot(A, X[k-1])\n",
    "        if (all( v == 0 for v in r)): # The algorithm converged\n",
    "            X[k:] = X[k-1]\n",
    "            return X\n",
    "            break\n",
    "        alpha = np.dot(np.transpose(r), r)/np.dot(np.transpose(r), np.dot(A, r))\n",
    "        X[k] = X[k-1] + alpha*r\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our algorithm! But first, let's borrow a function to generate a random symmetric positive-definite matrix, kindly provided by the previous notebook, and another one to calculate the vectorized euclidean metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Randomly generates an nxn symmetric positive-\n",
    "definite matrix A.\n",
    "\"\"\"\n",
    "def generate_spd_matrix(n):\n",
    "    A = np.random.random((n,n))\n",
    "    #constructing symmetry\n",
    "    A += A.T\n",
    "    #symmetric+diagonally dominant -> symmetric positive-definite\n",
    "    deltas = 0.1*np.random.random(n)\n",
    "    row_sum = A.sum(axis=1)-np.diag(A)\n",
    "    np.fill_diagonal(A, row_sum+deltas)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try our algorithm with some matrices of different sizes, and we'll compare it with the solution given by Numpy's solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = generate_spd_matrix(3)\n",
    "b3 = np.ones(3)\n",
    "x30 = np.zeros(3)\n",
    "\n",
    "X = gradient_descent(A3, b3, x30, 15)\n",
    "sol = np.linalg.solve(A3, b3)\n",
    "print (X[-1])\n",
    "print (sol)\n",
    "print (np.linalg.norm(X[-1] - sol)) # difference bewteen gradient_descent's solution and Numpy's solver's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A10 = generate_spd_matrix(10)\n",
    "b10 = np.ones(10)\n",
    "x100 = np.zeros(10)\n",
    "\n",
    "X = gradient_descent(A10, b10, x100, 15)\n",
    "sol = np.linalg.solve(A10, b10)\n",
    "print (X[-1])\n",
    "print (sol)\n",
    "print (np.linalg.norm(X[-1] - sol)) # difference bewteen gradient_descent's solution and Numpy's solver's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A50 = generate_spd_matrix(50)\n",
    "b50 = np.ones(50)\n",
    "x500 = np.zeros(50)\n",
    "\n",
    "X = gradient_descent(A50, b50, x500, 15)\n",
    "sol = np.linalg.solve(A50, b50)\n",
    "print (X[-1])\n",
    "print (sol)\n",
    "print (np.linalg.norm(X[-1] - sol)) # difference bewteen gradient_descent's solution and Numpy's solver's solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we're getting good solutions with 15 iterations, even for matrices on the bigger side. However, this method is not used too often; rather, its younger sibling, the Conjugate Gradient Method, is the more preferred choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='CGM' />\n",
    "## Conjugate Gradient Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works by succesively eliminating the $n$ orthogonal components of the error, one by one. The method arrives at the solution with the following finite loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, x0):\n",
    "    n = A.shape[0]\n",
    "    X = np.empty((n, n))\n",
    "    d = b - np.dot(A, x0)\n",
    "    R = np.empty((n, n))\n",
    "    X[0] = x0\n",
    "    R[0] = b - np.dot(A, x0)\n",
    "\n",
    "    for k in range(1, n):\n",
    "        if (all( v == 0 for v in R[k-1])): # The algorithm converged\n",
    "            X[k:] = X[k-1]\n",
    "            return X\n",
    "            break\n",
    "        alpha = np.dot(np.transpose(R[k-1]), R[k-1]) / np.dot(np.transpose(d), np.dot(A, d))\n",
    "        X[k] = X[k-1] + alpha*d\n",
    "        R[k] = R[k-1] - alpha*np.dot(A, d)\n",
    "        beta = np.dot(np.transpose(R[k]), R[k])/np.dot(np.transpose(R[k-1]), R[k-1])\n",
    "        d = R[k] + beta*d\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The science behind this algorithm is a bit long to explain, but for the curious ones, the explanation is on the official textbook (Numerical Analysis, 2nd Edition, Timothy Sauer). Now let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = generate_spd_matrix(3)\n",
    "b3 = np.ones(3)\n",
    "x30 = np.zeros(3)\n",
    "\n",
    "X = conjugate_gradient(A3, b3, x30)\n",
    "sol = np.linalg.solve(A3, b3)\n",
    "print (X[-1])\n",
    "print (sol)\n",
    "print (np.linalg.norm(X[-1]- sol)) # difference bewteen conjugate_gradient's solution and Numpy's solver's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A50 = generate_spd_matrix(50)\n",
    "b50 = np.ones(50)\n",
    "x500 = np.zeros(50)\n",
    "\n",
    "X = conjugate_gradient(A50, b50, x500)\n",
    "sol = np.linalg.solve(A50, b50)\n",
    "print (X[-1])\n",
    "print (sol)\n",
    "print (np.linalg.norm(X[-1]-sol)) # difference bewteen conjugate_gradient's solution and Numpy's solver's solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A100 = generate_spd_matrix(100)\n",
    "b100 = np.ones(100)\n",
    "x1000 = np.zeros(100)\n",
    "\n",
    "X = conjugate_gradient(A100, b100, x1000)\n",
    "sol = np.linalg.solve(A100, b100)\n",
    "print (X[-1])\n",
    "print (sol)\n",
    "print (np.linalg.norm(X[-1] - sol)) # difference bewteen conjugate_gradient's solution and Numpy's solver's solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for small matrices the error for `gradient_descent` is somewhat smaller than the error for `conjugate_gradient`, but for big matrices this method has an extremely small error, practically zero. Isn't that amazing?!\n",
    "\n",
    "Here are some questions for the student to think about:\n",
    "* In which cases can the Conjugate Gradient Method converge in less than $n$ iterations?\n",
    "* What will happen if you use the Gradient Descent or Conjugate Gradient Method with non-symmetric, non-positive-definite matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='CGMp' />\n",
    "## Conjugate Gradient Method with Preconditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that the Conjugate Gradient Method works very well, but can we make it better? \n",
    "\n",
    "Very often, the convergence rate of iterative methods depends on the condition number of matrix $A$. By preconditioning, we'll reduce the condition number of the problem. The preconditioned version of the problem $A\\,x = b$ is: $$M^{-1}\\,A\\,x = M^{-1}\\,b$$ The matrix $M$ must be as close to $A$ as possible and easy to invert. One simple choice is the Jacobi Preconditioner $M = D$, since it shares its diagonal with $A$ and, as a diagonal matrix, is easy to invert. By applying this modification, we'll find that the method converges even faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag_dot(D, v):\n",
    "    n = len(D)\n",
    "    sol = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        sol[i] = D[i] * v[i]\n",
    "    return sol\n",
    "\n",
    "def conjugate_gradient_J(A, b, x0):\n",
    "    M = np.diag(A)\n",
    "    M_p = M**-1\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    X = np.empty((n, n))\n",
    "    Z = np.empty((n, n))\n",
    "    R = np.empty((n, n))\n",
    "    X[0] = x0\n",
    "    R[0] = b - np.dot(A, x0)\n",
    "    Z[0] = diag_dot(M_p, R[0])\n",
    "    d = diag_dot(M_p, R[0])\n",
    "\n",
    "    for k in range(1, n):\n",
    "        if (all( v == 0 for v in R[k-1]) and pr): # The algorithm converged\n",
    "            X[k:] = X[k-1]\n",
    "            return X\n",
    "            break\n",
    "        alpha = np.dot(np.transpose(R[k-1]), Z[k-1]) / np.dot(np.transpose(d), np.dot(A, d))\n",
    "        X[k] = X[k-1] + alpha*d\n",
    "        R[k] = R[k-1] - alpha*np.dot(A, d)\n",
    "        Z[k] = diag_dot(M_p, R[k])\n",
    "        beta = np.dot(np.transpose(R[k]), Z[k])/np.dot(np.transpose(R[k-1]), Z[k-1])\n",
    "        d = Z[k] + beta*d\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aj100 = generate_spd_matrix(100)\n",
    "bj100 = np.ones(100)\n",
    "xj1000 = np.zeros(100)\n",
    "\n",
    "X = conjugate_gradient_J(Aj100, bj100, xj1000)\n",
    "X2 = conjugate_gradient(Aj100, bj100, xj1000)\n",
    "sol = np.linalg.solve(Aj100, bj100)\n",
    "print (np.linalg.norm(X[-1]- sol)) # difference bewteen gradient_descent's solution and Numpy's solver's solution\n",
    "print (np.linalg.norm(X2[-1]- sol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute errors for both are very much alike and practically zero, but the difference is the _speed_ with which the error decreases, as we'll see in the Exercises section of this notebook.\n",
    "\n",
    "Can you think of other preconditioners and try them out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='LP' />\n",
    "## Let's Play: Practical Exercises and Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, define a function to calculate the progress of the relative error for a given method, that is, input the array of approximate solutions `X` and the real solution provided by Numpy's solver `r_sol` and return an array with the relative error for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(X, r_sol):\n",
    "    n_steps = X.shape[0]\n",
    "    n_r_sol = np.linalg.norm(r_sol)\n",
    "    E = np.zeros(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        E[i] = np.linalg.norm(X[i] - r_sol) / n_r_sol\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the three methods with a small non-symmetric, non-positive-definite matrix. Plot the relative error for all three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "B = 10 * np.random.random((n,n))\n",
    "b = 10 * np.random.random(n)\n",
    "x0 = np.zeros(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = gradient_descent(B, b, x0, n)\n",
    "X2 = conjugate_gradient(B, b, x0)\n",
    "X3 = conjugate_gradient_J(B, b, x0)\n",
    "r_sol = np.linalg.solve(B, b)\n",
    "\n",
    "E1 = relative_error(X1, r_sol)\n",
    "E2 = relative_error(X2, r_sol)\n",
    "E3 = relative_error(X3, r_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.linspace(1, n, n)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.title('Evolution of the Relative Error for each method')\n",
    "plt.semilogy(iterations, E1, 'go', markersize=8) # Green spots are for Gradient Descent\n",
    "plt.semilogy(iterations, E2, 'ro', markersize=8) # Red spots are for Conjugate Gradient\n",
    "plt.semilogy(iterations, E3, 'co', markersize=8) # Cyan spots are for Conjugate Gradient with Jacobi Preconditioner\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, if the matrix doesn't meet the requirements for these methods, the results can be quite terrible.\n",
    "\n",
    "Let's try again, this time using an appropriate matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "A = 10 * generate_spd_matrix(n)\n",
    "b = 10 * np.random.random(n)\n",
    "x0 = np.random.random(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = gradient_descent(A, b, x0, n)\n",
    "X2 = conjugate_gradient(A, b, x0)\n",
    "X3 = conjugate_gradient_J(A, b, x0)\n",
    "r_sol = np.linalg.solve(A, b)\n",
    "\n",
    "E1 = relative_error(X1, r_sol)\n",
    "E2 = relative_error(X2, r_sol)\n",
    "E3 = relative_error(X3, r_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.linspace(1, n, n)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.title('Evolution of the Relative Error for each method')\n",
    "plt.semilogy(iterations, E1, 'go', markersize=4) # Green spots are for Gradient Descent\n",
    "plt.semilogy(iterations, E2, 'ro', markersize=4) # Red spots are for Conjugate Gradient\n",
    "plt.semilogy(iterations, E3, 'co', markersize=4) # Cyan spots are for Conjugate Gradient with Jacobi Preconditioner\n",
    "plt.grid(True)\n",
    "plt.xlim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! We started with a huge relative error and reduced it to practically zero in just under 6 iterations (the algorithms all have 100 iterations but we're showing you the first 10). We can clearly see that the Conjugate Gradient Method with Preconditioning needs the least of the three, with the Gradient Descent needing the most.\n",
    "\n",
    "Let's try with an even bigger matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "A = 10 * generate_spd_matrix(n)\n",
    "b = 10 * np.random.random(n)\n",
    "x0 = np.random.random(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = gradient_descent(A, b, x0, n)\n",
    "X2 = conjugate_gradient(A, b, x0)\n",
    "X3 = conjugate_gradient_J(A, b, x0)\n",
    "r_sol = np.linalg.solve(A, b)\n",
    "\n",
    "E1 = relative_error(X1, r_sol)\n",
    "E2 = relative_error(X2, r_sol)\n",
    "E3 = relative_error(X3, r_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.linspace(1, n, n)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Relative Error')\n",
    "plt.title('Evolution of the Relative Error for each method')\n",
    "plt.semilogy(iterations, E1, 'go', markersize=4) # Green spots are for Gradient Descent\n",
    "plt.semilogy(iterations, E2, 'ro', markersize=4) # Red spots are for Conjugate Gradient\n",
    "plt.semilogy(iterations, E3, 'co', markersize=4) # Cyan spots are for Conjugate Gradient with Jacobi Preconditioner\n",
    "plt.grid(True)\n",
    "plt.xlim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, reached a certain size for the matrix, the amount of iterations needed to reach a small error remains more or less the same. We encourage you to try other kinds of matrices to see how the algorithms behave, and experiment with the codes to your liking. Now let's move on to profiling.\n",
    "\n",
    "Of course, you win some, you lose some. Accelerating the convergence of the algorithm means you have to spend more of other resources. We'll use the functions `%timeit` and `%memit` to see how the algorithms behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = generate_spd_matrix(100)\n",
    "b = np.ones(100)\n",
    "x0 = np.random.random(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit gradient_descent(A, b, x0, 100)\n",
    "%timeit conjugate_gradient(A, b, x0)\n",
    "%timeit conjugate_gradient_J(A, b, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does not seem to be working now, it needs to be updated.\n",
    "#%memit gradient_descent(A, b, x0, 100)\n",
    "#%memit conjugate_gradient(A, b, x0)\n",
    "#%memit conjugate_gradient_J(A, b, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see something interesting here: all three algorithms need the same amount of memory.\n",
    "\n",
    "What happened with the measure of time? Why is it so big for the algorithm that has the best convergence rate? Besides the end of the loop, we have one other criteria for stopping the algorithm: When the residue r reaches the _exact_ value of zero, we say that the algorithm converged, and stop. However it's very hard to get an error of zero for randomized initial guesses, so this almost never happens, and we can't take advantage of the convergence rate of the algorithms. \n",
    "\n",
    "There's a way we can fix this: instead of using this criteria, make the algorithm stop when a certain _tolerance_ is reached. That way, when the error gets small enough (which happens faster for the third method), we can stop and say that we got a good enough solution. We'll give the task of modifying the algorithms to let this happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try with different matrices, different initial conditions, different sizes, etcetera. Try some more plotting, profiling, and experimenting. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='acknowledgements' />\n",
    "# Acknowledgements\n",
    "* _Material created by professor Claudio Torres_ (`ctorres@inf.utfsm.cl`) _and assistants: Laura Bermeo, Alvaro Salinas, Axel Simonsen and Martín Villanueva. DI UTFSM. April 2016._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
